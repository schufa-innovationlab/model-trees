{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Classifiers\n",
    "This notebook compares multiple classification methods on multiple datasets and evaluates them in terms of the area under the roc-curve (roc-auc).\n",
    "\n",
    "__Note:__ There is a similar [notebook for regression datasets](Comparing_Regressors.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "try:\n",
    "    # Model Trees are installed / on the path\n",
    "    from modeltrees import ModelTreeClassifier\n",
    "except:\n",
    "    # Assume project structure\n",
    "    import sys\n",
    "    sys.path.append(\"..\")\n",
    "    from modeltrees import ModelTreeClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Downloading and accessing files\n",
    "import shutil\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Specific Data Formats\n",
    "from scipy.io.arff import loadarff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datasets\n",
    "In this section, all datasets for the comparison are defined. Missing datasets are downloaded automatically.\n",
    "\n",
    "See [Section 3.3](#characteristics) for a list of dataset characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Downloading Datasets\n",
    "We do not ship datasets with this repository, but the notebook will automatically try to download missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data\"\n",
    "\n",
    "def copy_from_zip(zip_file, path_in_zip, dest_path):\n",
    "    # Open Zip File\n",
    "    with zipfile.ZipFile(zip_file) as zf:\n",
    "        # open the destination path for writing\n",
    "        with open(dest_path, \"wb\") as f:\n",
    "            f.write(zf.read(path_in_zip)) \n",
    "            \n",
    "            \n",
    "def get_dataset_file_path(dataset_id, url, path_in_zip=None, zip_file=None, file=None):\n",
    "    # Two Cases:\n",
    "    #      (a) Online Source has the zipped file\n",
    "    #      (b) Online Source has the plain file\n",
    "    is_zipped = (path_in_zip is not None)\n",
    "    \n",
    "    # If no file is specified, take the name from the url\n",
    "    if file is None:\n",
    "        if is_zipped:\n",
    "            file = os.path.basename(path_in_zip)\n",
    "        else:\n",
    "            file = urlparse(url)\n",
    "            file = os.path.basename(file.path)\n",
    "     \n",
    "    # Create path to local file\n",
    "    path = Path(data_path, dataset_id, file)\n",
    "    \n",
    "    if not path.exists():\n",
    "        # Create missing folders\n",
    "        os.makedirs(path.parent, exist_ok=True)\n",
    "        \n",
    "        if is_zipped:\n",
    "            # Download zip file (if missing)\n",
    "            path_to_zip = get_dataset_file_path(dataset_id, url, file=zip_file)\n",
    "            \n",
    "            # Extract file\n",
    "            copy_from_zip(path_to_zip, path_in_zip, path)\n",
    "        else:\n",
    "            # Download missing file\n",
    "            with urllib.request.urlopen(url) as response:\n",
    "                with open(path, \"wb\") as outputFile:\n",
    "                    shutil.copyfileobj(response, outputFile)\n",
    "    \n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset Definitions\n",
    "The following defines different datasets with their download url, and possibly some preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_bankruptcy():\n",
    "    ds_name = \"bankruptcy\"\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip\"\n",
    "\n",
    "    # Load arff\n",
    "    X = []\n",
    "    y = []\n",
    "    for year in range(1, 6):\n",
    "        path = get_dataset_file_path(ds_name, url, path_in_zip=f\"{year}year.arff\")\n",
    "\n",
    "        D, _ = loadarff(path)\n",
    "        y.append( D[\"class\"].astype(np.int8) )\n",
    "        X.append( np.asarray([list(row) for row in D[list(D.dtype.names)[:-1]]], dtype=np.float) )\n",
    "\n",
    "    X = np.concatenate(X, axis=0)\n",
    "    y = np.concatenate(y, axis=0)\n",
    "    \n",
    "    return X, y, {'ref':'https://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Iterating over all Datasets\n",
    "This gives a generator that iterates over all datasets.  \n",
    "Each dataset is a triple consisting of \n",
    "- Features Matrix `X`, \n",
    "- Target Vector `y`, and \n",
    "- An attribute dictionary that contains meta information like the name of the dataset or a reference url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    # Using generators instead of lists for memory efficiency reasons.\n",
    "    \n",
    "    # Dataset 1: Bankruptcy\n",
    "    X, y, attr = fetch_bankruptcy()\n",
    "    attr['name'] = 'Bankruptcy' \n",
    "    yield (X, y, attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classifiers\n",
    "We are comparing the following regressors:\n",
    "- Logistic Regression\n",
    "- Decision Trees with maximal depth 3 and 6 \n",
    "- Model Trees with maximal depth 1 and 3. We compare two split criteria:\n",
    "    - Plain Gradients \n",
    "    - Renormalized Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifiers():\n",
    "    return [\n",
    "        (LogisticRegression(solver=\"liblinear\"), \"Log. Reg.\"),\n",
    "        (DecisionTreeClassifier(max_depth=3, random_state=12), \"Decision Tree\"),\n",
    "        (DecisionTreeClassifier(max_depth=6, random_state=12), \"Decision Tree\"),\n",
    "        (ModelTreeClassifier(max_depth=1), \"Model Tree (Gradient)\"),\n",
    "        (ModelTreeClassifier(max_depth=3), \"Model Tree (Gradient)\"),\n",
    "        (ModelTreeClassifier(max_depth=1, criterion=\"gradient-renorm-z\"), \"Model Tree (Renorm. Gradient)\"),\n",
    "        (ModelTreeClassifier(max_depth=3, criterion=\"gradient-renorm-z\"), \"Model Tree (Renorm. Gradient)\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison\n",
    "### 3.1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation: Number of Folds\n",
    "n_fold = 5\n",
    "\n",
    "seed = 42   # We suggest to try other values to get a feeling for the stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evaluation\n",
    "Iterating over datasets and regressors and evaluating the regressors in terms of the $r^2$ metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame for results (see 3.4)\n",
    "# Multi-Index for better readability\n",
    "col_index = pd.MultiIndex(levels=[[],[]],\n",
    "                             codes=[[],[]],\n",
    "                             names=['Method', 'Depth'])\n",
    "results = pd.DataFrame(columns=col_index)\n",
    "\n",
    "# Create a DataFrame for the Dataset Characteristics (see 3.3)\n",
    "ds_characteristics = pd.DataFrame(columns=(\"#Samples\", \"#Features\", \"Reference\"))\n",
    "\n",
    "# Create a scorer function\n",
    "scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "\n",
    "# Iterate over Datasets\n",
    "for X, y, attr in get_datasets():\n",
    "    ds_name = attr['name']\n",
    "    \n",
    "    # Store dataset  characteristics\n",
    "    n_samples, n_features = X.shape\n",
    "    ds_characteristics.loc[ds_name, \"#Samples\"] = n_samples\n",
    "    ds_characteristics.loc[ds_name, \"#Features\"] = n_features\n",
    "    \n",
    "    if \"ref\" in attr:\n",
    "        ds_characteristics.loc[ds_name, \"Reference\"] = attr[\"ref\"]\n",
    "    else:\n",
    "        ds_characteristics.loc[ds_name, \"Reference\"] = None\n",
    "    \n",
    "    # Iterate over Regressors\n",
    "    for model, m_name in get_classifiers():\n",
    "        # Use the same seed for comparing different regressors\n",
    "        kfold = KFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "        \n",
    "        # Build processing pipeline\n",
    "        model_pipe = Pipeline([('Normalize', QuantileTransformer()), ('Impute', SimpleImputer()), ('Predict', model)])\n",
    "        \n",
    "        # Evalute over the folds\n",
    "        scores = cross_val_score(model_pipe, X, y, scoring=scorer, cv=kfold)\n",
    "        \n",
    "        # Compute statistics from list of scores\n",
    "        mean_score = np.mean(scores)\n",
    "        std_score = np.std(scores)\n",
    "        \n",
    "        # Create result cell\n",
    "        cell_text = f\"{mean_score*100:.2f} ± {std_score*100:.2f}\" \n",
    "        \n",
    "        # Multi-Indexing\n",
    "        if hasattr(model, 'max_depth'):\n",
    "            col_idx = (m_name, f\"{model.max_depth}\")\n",
    "        else:\n",
    "            col_idx = (m_name, '-')\n",
    "            \n",
    "        results.loc[ds_name, col_idx] = cell_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Dataset Characteristics <a id='characteristics'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_1685247a_1a99_11ea_b710_10e7c62ae0ed\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >#Samples</th>        <th class=\"col_heading level0 col1\" >#Features</th>        <th class=\"col_heading level0 col2\" >Reference</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_1685247a_1a99_11ea_b710_10e7c62ae0edlevel0_row0\" class=\"row_heading level0 row0\" >Bankruptcy</th>\n",
       "                        <td id=\"T_1685247a_1a99_11ea_b710_10e7c62ae0edrow0_col0\" class=\"data row0 col0\" >43405</td>\n",
       "                        <td id=\"T_1685247a_1a99_11ea_b710_10e7c62ae0edrow0_col1\" class=\"data row0 col1\" >64</td>\n",
       "                        <td id=\"T_1685247a_1a99_11ea_b710_10e7c62ae0edrow0_col2\" class=\"data row0 col2\" ><a target=\"_blank\" href=\"https://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data\">Link</a></td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1ba21464470>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_characteristics[\"#Samples\"] = ds_characteristics[\"#Samples\"].astype(dtype=np.int)\n",
    "ds_characteristics[\"#Features\"] = ds_characteristics[\"#Features\"].astype(dtype=np.int)\n",
    "\n",
    "def format_link(val):\n",
    "    # Handle Empty references\n",
    "    if val is None:\n",
    "        return ''\n",
    "    \n",
    "    # Format link\n",
    "    return '<a target=\"_blank\" href=\"{}\">Link</a>'.format(val)\n",
    "\n",
    "ds_characteristics.style.format({'Reference': format_link})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Results\n",
    "The classifiers are evaluated in terms of the roc-auc metric (area under the roc-curve).  \n",
    "The following results are given in percentage. The uncertainty is given as standard deviation of the roc-auc score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th>Log. Reg.</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Decision Tree</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Model Tree (Gradient)</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Model Tree (Renorm. Gradient)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Depth</th>\n",
       "      <th>-</th>\n",
       "      <th>3</th>\n",
       "      <th>6</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bankruptcy</th>\n",
       "      <td>81.07 ± 0.76</td>\n",
       "      <td>75.65 ± 1.15</td>\n",
       "      <td>84.27 ± 1.80</td>\n",
       "      <td>88.76 ± 0.80</td>\n",
       "      <td>91.59 ± 0.40</td>\n",
       "      <td>88.74 ± 0.82</td>\n",
       "      <td>91.22 ± 0.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Method         Log. Reg. Decision Tree               Model Tree (Gradient)  \\\n",
       "Depth                  -             3             6                     1   \n",
       "Bankruptcy  81.07 ± 0.76  75.65 ± 1.15  84.27 ± 1.80          88.76 ± 0.80   \n",
       "\n",
       "Method                   Model Tree (Renorm. Gradient)                \n",
       "Depth                  3                             1             3  \n",
       "Bankruptcy  91.59 ± 0.40                  88.74 ± 0.82  91.22 ± 0.61  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
